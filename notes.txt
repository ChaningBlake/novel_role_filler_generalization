-----------------------------------------------
-- Meeting                                   --
-----------------------------------------------
Switched to SGD and huber loss
    improved with SGD and even better with huber loss
    I can use lr of .1 now
    
Got the model exported

I created a lexicon of 10 words, but don't we need more words to train the encoder decoder?


Abstraction needs less data

-------------------------------------------
can put none in for the length
data still needs to have a fixed shape, but the models will adapt to input
100,000 word corpus
regularization methods - any attempt to make the model generalize better -> covering up a letter
maybe add a dropout layer
batch size - 32
DO Mike's first

DISCUSSION - we could use other models for the words
    word to vec
    bert models
    
-----------------------------------------------
-- Notes                                     --
-----------------------------------------------
* SGD Optimizer
* Huber Loss
* Train up encoder on 10 word lexicon



-----------------------------------------------
-- Workflow                                  --
-----------------------------------------------

1. testSentences.r
2. cat aboveOutput | ./combo2letters.py 200 100
3. splitFiles.sh
4. ./word-enc-dec.py trainset testset 6






-----------------------------------------------
-- Notes on Mike's Code                      --
-----------------------------------------------
He did 200 task blocks
a task block is 200 times
block accuracy is block_tasks_correct / 200

Querying is done by a random selection

correct trial on line 505

block task correct if it get's the whole sequence correct
	(line 513)

loops until greater than 95 percent accuracy

Taks Complete:
Block Accuracy:

Final Block Accuracy:                <-- block_tasks_correct/200 *100
Generalization Accuracy:             <-- This is the test set