-----------------------------------------------
-- Meeting                                   --
-----------------------------------------------

Check the vectors yourself

Chart the sum squared errors. Are there any spikes?

core model - start with outer encoder decoder | the pretrained models
            then e2e
            - e2e last
    other flow
    
SHOW THE HOLE - THen show how we will fill it. First time fill the assumed action and comparing
Show the natural next step with e2e.









four time steps and the last one produces the stop token with a bunch of zeros
shelve data structure -> Data structure like a dictionary but on disk
-> Tieing the words to the string
--> Keep everything faster so you don't have to encode every time.



****
Start/Stop output should be sigmoid
Usually use binary cross entropy as loss function

supply loss function as list to give different layers different functions




For Outer Encoder-Decoder
--------------------------------------------------------
*Have to use linear output. Because they are continuous vectors.
*Make sure you use mean squared error as well
--- This is because of the continuous representations


-------------------------------------------
regularization methods - any attempt to make the model generalize better -> covering up a letter
maybe add a dropout layer
batch size - 32
DISCUSSION - we could use other models for the words
    word to vec
    bert models
    
Abstraction needs less data
    
-----------------------------------------------
-- Notes                                     --
-----------------------------------------------



-----------------------------------------------
-- Workflow                                  --
-----------------------------------------------

1. testSentences.r
2. cat aboveOutput | ./combo2letters.py 200 100
3. splitFiles.sh
4. ./word-enc-dec.py trainset testset 6






-----------------------------------------------
-- Notes on Mike's Code                      --
-----------------------------------------------
He did 200 task blocks
a task block is 200 times
block accuracy is block_tasks_correct / 200

Querying is done by a random selection

correct trial on line 505

block task correct if it get's the whole sequence correct
	(line 513)

loops until greater than 95 percent accuracy

Taks Complete:
Block Accuracy:

Final Block Accuracy:                <-- block_tasks_correct/200 *100
Generalization Accuracy:             <-- This is the test set
