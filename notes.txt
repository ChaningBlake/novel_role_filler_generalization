-----------------------------------------------
-- Meeting                                   --
-----------------------------------------------

Bar graph with fractions of correct
mean performance and standard error

box plots don't assume normality
but a lot of times we assume normality










IF multiple gates open, flip a coin for which one oto decode
Could average the outputs and feed them in
add them together and normalize them

Empty is two vectors of zeros. If others are none you shouldn't do the average


four time steps and the last one produces the stop token with a bunch of zeros

shelve data structure -> Data structure like a dictionary but on disk
-> Tieing the words to the string
--> Keep everything faster so you don't have to encode every time.



****
Start/Stop output should be sigmoid
Usually use binary cross entropy as loss function

supply loss function as list to give different layers different functions




For Outer Encoder-Decoder
--------------------------------------------------------
*Have to use linear output. Because they are continuous vectors.
*Make sure you use mean squared error as well
--- This is because of the continuous representations


-------------------------------------------
regularization methods - any attempt to make the model generalize better -> covering up a letter
maybe add a dropout layer
batch size - 32
DISCUSSION - we could use other models for the words
    word to vec
    bert models
    
Abstraction needs less data
    
-----------------------------------------------
-- Notes                                     --
-----------------------------------------------



-----------------------------------------------
-- Workflow                                  --
-----------------------------------------------

1. testSentences.r
2. cat aboveOutput | ./combo2letters.py 200 100
3. splitFiles.sh
4. ./word-enc-dec.py trainset testset 6






-----------------------------------------------
-- Notes on Mike's Code                      --
-----------------------------------------------
He did 200 task blocks
a task block is 200 times
block accuracy is block_tasks_correct / 200

Querying is done by a random selection

correct trial on line 505

block task correct if it get's the whole sequence correct
	(line 513)

loops until greater than 95 percent accuracy

Taks Complete:
Block Accuracy:

Final Block Accuracy:                <-- block_tasks_correct/200 *100
Generalization Accuracy:             <-- This is the test set
