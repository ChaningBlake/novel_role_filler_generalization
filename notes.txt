-----------------------------------------------
-- Meeting                                   --
-----------------------------------------------

Bar graph with fractions of correct
mean performance and standard error

box plots don't assume normality
but a lot of times we assume normality

run ten models on each

assumptions and usage on wikipedia page

seperate the data and the code that makes the plots












IF multiple gates open, flip a coin for which one oto decode
Could average the outputs and feed them in
add them together and normalize them

Empty is two vectors of zeros. If others are none you shouldn't do the average


four time steps and the last one produces the stop token with a bunch of zeros

shelve data structure -> Data structure like a dictionary but on disk
-> Tieing the words to the string
--> Keep everything faster so you don't have to encode every time.




Questions:
Decoder hidden layer returns sequences. Why? Difference with return state?
Start token at the same time as the first tokens right?
No! Goes by itself, give it zero for everything else
PostY has stop token
All zeros for the word once you split it the model


******
S/S output should be sigmoid
Usually use binary cross entropy as loss function

supply loss function as list to give different layers different functions







For Outer Encoder-Decoder
--------------------------------------------------------
*Have to use linear output. Because they are continuous vectors.
*Make sure you use mean squared error as well
--- This is because of the continuous representations

---How do we asses how it's doing?
Calculate eucidean distance between what you get and the target, and all potential vectors. Whichever has the smallest distance is the one we say it was trying to output.




Recurrecy doesn't need a timing signal.  


-------------------------------------------
regularization methods - any attempt to make the model generalize better -> covering up a letter
maybe add a dropout layer
batch size - 32
DISCUSSION - we could use other models for the words
    word to vec
    bert models
    
Abstraction needs less data
    
-----------------------------------------------
-- Notes                                     --
-----------------------------------------------
* SGD Optimizer
* Huber Loss
* Train up encoder on 10 word lexicon



-----------------------------------------------
-- Workflow                                  --
-----------------------------------------------

1. testSentences.r
2. cat aboveOutput | ./combo2letters.py 200 100
3. splitFiles.sh
4. ./word-enc-dec.py trainset testset 6






-----------------------------------------------
-- Notes on Mike's Code                      --
-----------------------------------------------
He did 200 task blocks
a task block is 200 times
block accuracy is block_tasks_correct / 200

Querying is done by a random selection

correct trial on line 505

block task correct if it get's the whole sequence correct
	(line 513)

loops until greater than 95 percent accuracy

Taks Complete:
Block Accuracy:

Final Block Accuracy:                <-- block_tasks_correct/200 *100
Generalization Accuracy:             <-- This is the test set
