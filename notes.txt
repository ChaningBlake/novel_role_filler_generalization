-----------------------------------------------
-- Meeting                                   --
-----------------------------------------------
Switched to SGD and huber loss
    improved with SGD and even better with huber loss
    I can use lr of .1 now

I created a lexicon of 10 words, but don't we need more words to train the encoder decoder?

Is it okay that I export the model as a JSON object? it works!s
-----------------------------------------------
-- Notes                                     --
-----------------------------------------------
* SGD Optimizer
* Huber Loss
* Train up encoder on 10 word lexicon



-----------------------------------------------
-- Workflow                                  --
-----------------------------------------------

1. testSentences.r
2. cat aboveOutput | ./combo2letters.py 200 100
3. splitFiles.sh
4. ./word-enc-dec.py trainset testset 6






-----------------------------------------------
-- Notes on Mike's Code                      --
-----------------------------------------------
He did 200 task blocks
a task block is 200 times
block accuracy is block_tasks_correct / 200

Querying is done by a random selection

correct trial on line 505

block task correct if it get's the whole sequence correct
	(line 513)

loops until greater than 95 percent accuracy

Taks Complete:
Block Accuracy:

Final Block Accuracy:                <-- block_tasks_correct/200 *100
Generalization Accuracy:             <-- This is the test set